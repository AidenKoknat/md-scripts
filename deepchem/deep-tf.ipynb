{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that GPU is available to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "len(tf.config.list_physical_devices('GPU'))\n",
    "# assert tf.test.is_gpu_available()\n",
    "# assert tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "1204\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load dataset\n",
    "pathway = Path()\n",
    "sigma_csv = \"../datasets/VT_2005_sigma_smiles_cid.csv\"\n",
    "sigma_csv = pd.read_csv(sigma_csv)\n",
    "sigma_df = pd.DataFrame(sigma_csv)\n",
    "smile = sigma_df[\"SMILE\"]\n",
    "smile_list = smile.tolist()\n",
    "vocab_smile = set()\n",
    "index = sigma_df[\"Index\"]\n",
    "txt_file_path = \"../datasets/VT-2005_Sigma_Profiles_v2\"\n",
    "dict_sigma = {}\n",
    "counter = 0\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_smiles = test_dataset.ids\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "    \n",
    "    \n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 1024\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "# create vocabulary of smiles\n",
    "# for x in smile_list:\n",
    "#     # print(x)\n",
    "#     for y in x:\n",
    "#         try:\n",
    "#             vocab_smile.add(y)\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "# print(vocab_smile)\n",
    "# vocab_smile = list(vocab_smile)\n",
    "# input_tokens = sorted(vocab_smile)\n",
    "\n",
    "# create a dictionary of indices and corresponding sigma profiles\n",
    "for i in index:\n",
    "    i_str = str(i)\n",
    "    i_str = i_str.zfill(4)\n",
    "    for file in pathway.glob(f\"{txt_file_path}/VT2005-{i_str}-PROF.txt\"):\n",
    "        # print(file)\n",
    "        sigma_file = pd.read_csv(file, sep='\\s+', header=None)\n",
    "        sigma_df = pd.DataFrame(sigma_file)\n",
    "        sigma_1 = sigma_df[1]\n",
    "        sigma1_np = sigma_1.to_numpy()\n",
    "        dict_sigma[i_str] = list(sigma1_np)\n",
    "        # print(sigma1_np)\n",
    "        # print(dict_sigma)\n",
    "        counter +=1\n",
    "\n",
    "print(counter)\n",
    "# the dataset for VT-2005 index and corresponding sigma-profile\n",
    "dataset_index = [key for key,val in dict_sigma.items()]\n",
    "dataset_sigma = [val for key,val in dict_sigma.items()]\n",
    "assert len(dataset_sigma) == len(dataset_index)  # verify that the lengths are equal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BasicSmilesTokenizer' from 'deepchem.feat' (/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/feat/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepchem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasicSmilesTokenizer\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BasicSmilesTokenizer()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCC(=O)OC1=CC=CC=C1C(=O)O\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BasicSmilesTokenizer' from 'deepchem.feat' (/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/feat/__init__.py)"
     ]
    }
   ],
   "source": [
    "# from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "from deepchem.feat import BasicSmilesTokenizer\n",
    "\n",
    "tokenizer = BasicSmilesTokenizer()\n",
    "print(tokenizer.tokenize(\"CC(=O)OC1=CC=CC=C1C(=O)O\"))\n",
    "model.predict_embeddings(tokenizer.tokenize(\"CC(=O)OC1=CC=CC=C1C(=O)O\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "\n",
    "predicted = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on MUV dataset) on the VT-2005 SMILES dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 13:18:51.233800: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 13:23:36.874770: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-01 13:23:36.894202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-01 13:23:37.835359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:af:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-04-01 13:23:37.835445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-01 13:23:38.161176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-01 13:23:38.161352: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-01 13:23:38.212539: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-01 13:23:38.328143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-01 13:23:38.329320: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scif/apps/deepchem250gpu/lib:/opt/ohpc/pub/utils/ccs/cuda/11.2.0_460.27.04/toolkit/lib64:/opt/ohpc/pub/libs/ccs/singularity/3.2.0/lib:/project/qsh226_uksr/DES_usman/plumed-2.7.3/lib:/.singularity.d/libs\n",
      "2022-04-01 13:23:38.436822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-01 13:23:38.449502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-01 13:23:38.449545: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-01 13:23:38.450386: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-01 13:23:38.450472: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-01 13:23:38.450494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-01 13:23:38.450499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2', '4', 'r', '1', '#', 'C', 'O', '=', '[', 'H', 'l', 'S', '-', '3', 'B', 'I', ')', ']', 'F', '5', 'P', '+', 'N', '('}\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 13:23:41.545387: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-01 13:23:41.557922: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz\n",
      "714it [00:00, 1093766.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 41 of 714 validation SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained on MUV) on a list of smiles from VT-2005 and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load dataset\n",
    "pathway = Path()\n",
    "sigma_csv = \"../datasets/VT_2005_sigma_smiles_cid.csv\"\n",
    "sigma_csv = pd.read_csv(sigma_csv)\n",
    "sigma_df = pd.DataFrame(sigma_csv)\n",
    "smile = sigma_df[\"SMILE\"]\n",
    "smile_list = smile.tolist()\n",
    "vocab_smile = set()\n",
    "index = sigma_df[\"Index\"]\n",
    "txt_file_path = \"./datasets/VT-2005_Sigma_Profiles_v2\"\n",
    "dict_sigma = {}\n",
    "counter = 0\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_muv(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 100\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../muv-256_100epochs/muv-256-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "# create vocabulary of smiles\n",
    "for x in smile_list:\n",
    "    # print(x)\n",
    "    for y in x:\n",
    "        try:\n",
    "            vocab_smile.add(y)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(vocab_smile)\n",
    "vocab_smile = list(vocab_smile)\n",
    "input_tokens = sorted(vocab_smile)\n",
    "\n",
    "# create a dictionary of indices and corresponding sigma profiles\n",
    "for i in index:\n",
    "    i_str = str(i)\n",
    "    i_str = i_str.zfill(4)\n",
    "    for file in pathway.glob(f\"{txt_file_path}/VT2005-{i_str}-PROF.txt\"):\n",
    "        # print(file)\n",
    "        sigma_file = pd.read_csv(file, sep='\\s+', header=None)\n",
    "        sigma_df = pd.DataFrame(sigma_file)\n",
    "        sigma_1 = sigma_df[1]\n",
    "        sigma1_np = sigma_1.to_numpy()\n",
    "        dict_sigma[i_str] = list(sigma1_np)\n",
    "        # print(sigma1_np)\n",
    "        # print(dict_sigma)\n",
    "        counter +=1\n",
    "\n",
    "print(counter)\n",
    "# the dataset for VT-2005 index and corresponding sigma-profile\n",
    "dataset_index = [key for key,val in dict_sigma.items()]\n",
    "dataset_sigma = [val for key,val in dict_sigma.items()]\n",
    "assert len(dataset_sigma) == len(dataset_index)  # verify that the lengths are equal\n",
    "\n",
    "\n",
    "predictions = []\n",
    "good_smiles = []\n",
    "bad = False\n",
    "for smile in smile_list:\n",
    "    for i in smile:\n",
    "        if i not in tokens:\n",
    "            bad = True\n",
    "\n",
    "    if not bad:\n",
    "        #predictions.append(model.predict_from_sequences(smile))\n",
    "        good_smiles.append(smile)\n",
    "        # print(smile)\n",
    "    # else:\n",
    "    #     predictions.append(model.predict_from_sequences(smile_list))\n",
    "\n",
    "predictions = model.predict_from_sequences(good_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(good_smiles, predictions)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(good_smiles)} validation SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on CHEMBL25 dataset) on the VT-2005 SMILES dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "{'2', '4', 'r', '1', '#', 'C', 'O', '=', '[', 'H', 'l', 'S', '-', '3', 'B', 'I', ')', ']', 'F', '5', 'P', '+', 'N', '('}\n",
      "0\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function KerasModel._compute_model at 0x7fd29ac5cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function KerasModel._compute_model at 0x7fd29ac5cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1204it [00:00, 1053607.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 90 of 1204 validation SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# VT-2005 \n",
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained on chembl25) on a list of smiles from VT-2005 and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load dataset\n",
    "pathway = Path()\n",
    "sigma_csv = \"../datasets/VT_2005_sigma_smiles_cid.csv\"\n",
    "sigma_csv = pd.read_csv(sigma_csv)\n",
    "sigma_df = pd.DataFrame(sigma_csv)\n",
    "smile = sigma_df[\"SMILE\"]\n",
    "smile_list = smile.tolist()\n",
    "vocab_smile = set()\n",
    "index = sigma_df[\"Index\"]\n",
    "txt_file_path = \"../datasets/VT-2005_Sigma_Profiles_v2\"\n",
    "dict_sigma = {}\n",
    "counter = 0\n",
    "\n",
    "# tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "# train_dataset, valid_dataset, test_dataset = datasets\n",
    "# train_smiles = train_dataset.ids\n",
    "# valid_smiles = valid_dataset.ids\n",
    "# test_smiles = test_dataset.ids\n",
    "\n",
    "# Load CHEMBL25 from joblib files instead. Probably faster than molnet\n",
    "train_dataset = dc.utils.load_from_disk('../datasets/chembl25/chembl25_strat_train.joblib')\n",
    "train_smiles = train_dataset.ids\n",
    "valid_dataset = dc.utils.load_from_disk('../datasets/chembl25/chembl25_strat_valid.joblib')\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_dataset = dc.utils.load_from_disk('../datasets/chembl25/chembl25_strat_test.joblib')\n",
    "test_smiles = test_dataset.ids\n",
    "print(\"Train: \",len(train_smiles))\n",
    "print(\"Valid: \",len(valid_smiles))\n",
    "print(\"Test: \",len(test_smiles))\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 1024\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "# create vocabulary of smiles\n",
    "for x in smile_list:\n",
    "    # print(x)\n",
    "    for y in x:\n",
    "        try:\n",
    "            vocab_smile.add(y)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(vocab_smile)\n",
    "vocab_smile = list(vocab_smile)\n",
    "input_tokens = sorted(vocab_smile)\n",
    "\n",
    "# create a dictionary of indices and corresponding sigma profiles\n",
    "for i in index:\n",
    "    i_str = str(i)\n",
    "    i_str = i_str.zfill(4)\n",
    "    for file in pathway.glob(f\"{txt_file_path}/VT2005-{i_str}-PROF.txt\"):\n",
    "        # print(file)\n",
    "        sigma_file = pd.read_csv(file, sep='\\s+', header=None)\n",
    "        sigma_df = pd.DataFrame(sigma_file)\n",
    "        sigma_1 = sigma_df[1]\n",
    "        sigma1_np = sigma_1.to_numpy()\n",
    "        dict_sigma[i_str] = list(sigma1_np)\n",
    "        # print(sigma1_np)\n",
    "        # print(dict_sigma)\n",
    "        counter +=1\n",
    "\n",
    "print(counter)\n",
    "# the dataset for VT-2005 index and corresponding sigma-profile\n",
    "dataset_index = [key for key,val in dict_sigma.items()]\n",
    "dataset_sigma = [val for key,val in dict_sigma.items()]\n",
    "assert len(dataset_sigma) == len(dataset_index)  # verify that the lengths are equal\n",
    "\n",
    "\n",
    "predictions = []\n",
    "good_smiles = []\n",
    "bad = False\n",
    "for smile in smile_list:\n",
    "    for i in smile:\n",
    "        if i not in tokens:\n",
    "            bad = True\n",
    "\n",
    "    if not bad:\n",
    "        #predictions.append(model.predict_from_sequences(smile))\n",
    "        good_smiles.append(smile)\n",
    "        # print(smile)\n",
    "    # else:\n",
    "    #     predictions.append(model.predict_from_sequences(smile_list))\n",
    "\n",
    "predictions = model.predict_from_sequences(good_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(good_smiles, predictions)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(good_smiles)} validation SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CHEMBL25 dataset and save as joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [13:35:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:35:42] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:44:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:44:51] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:45:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:45:48] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:46:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:46:40] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/data/datasets.py:471: RuntimeWarning: overflow encountered in multiply\n",
      "  y_m2 += dy * (y - y_means)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1428330\n",
      "Valid 178543\n",
      "Test 178542\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train, valid, test = datasets\n",
    "dc.utils.save_to_disk(datasets, 'chembl25_strat.joblib')\n",
    "dc.utils.save_to_disk(train, 'chembl25_strat_train.joblib')\n",
    "dc.utils.save_to_disk(test, 'chembl25_strat_test.joblib')\n",
    "dc.utils.save_to_disk(valid, 'chembl25_strat_valid.joblib')\n",
    "train_dataset = dc.utils.load_from_disk('chembl25_strat_train.joblib')\n",
    "train_smiles = train_dataset.ids\n",
    "valid_dataset = dc.utils.load_from_disk('chembl25_strat_valid.joblib')\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_dataset = dc.utils.load_from_disk('chembl25_strat_test.joblib')\n",
    "test_smiles = test_dataset.ids\n",
    "print(\"Train\",len(train_smiles))\n",
    "print(\"Valid\",len(valid_smiles))\n",
    "print(\"Test\",len(test_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cc1cc(-c2csc(N=C(N)N)n2)cn1C'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on CHEMBL25 for 3 epochs) on the test/validation SMILES from CHEMBL25 dataset.\n",
    "Training set: 1,428,327 \n",
    "Test set: 178,541 \n",
    "Valid set: 178, 547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1428327\n",
      "Valid:  178547\n",
      "Test:  178541\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 12:43:19.865987: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-13 12:43:20.928561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-13 12:43:21.780627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-04-13 12:43:21.780679: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-13 12:43:22.230475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-13 12:43:22.230544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-13 12:43:22.323336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-13 12:43:22.424413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-13 12:43:22.427127: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scif/apps/deepchem250gpu/lib:/opt/ohpc/pub/utils/ccs/cuda/11.2.0_460.27.04/toolkit/lib64:/opt/ohpc/pub/libs/ccs/singularity/3.2.0/lib:/project/qsh226_uksr/DES_usman/plumed-2.7.3/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/libfabric/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/lib/release:/opt/ohpc/pub/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/compiler/lib/intel64_lin:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/libfabric/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib/release:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/ipp/lib/intel64:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mkl/lib/intel64_lin:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/tbb/lib/intel64/gcc4.8:/opt/ohpc/pub/intel/debugger_2020/python/intel64/lib:/opt/ohpc/pub/intel/debugger_2020/libipt/intel64/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/daal/lib/intel64_lin:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/.singularity.d/libs\n",
      "2022-04-13 12:43:22.590122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-13 12:43:22.612181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-13 12:43:22.612200: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-13 12:43:22.623993: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-13 12:43:22.624062: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-13 12:43:22.624080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-13 12:43:22.624084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "2022-04-13 12:43:26.829169: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-13 12:43:26.848009: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz\n",
      "178547it [00:00, 1256662.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 5994 of 178547 validation SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178541it [00:00, 1354058.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 5998 of 178541 test SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No VT-2005 \n",
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained on chembl25) and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_smiles = test_dataset.ids\n",
    "\n",
    "# Load CHEMBL25 from joblib files instead. Probably faster than molnet\n",
    "# train_dataset = dc.utils.load_from_disk('./chembl25_strat_train.joblib')\n",
    "# train_smiles = train_dataset.ids\n",
    "# valid_dataset = dc.utils.load_from_disk('./chembl25_strat_valid.joblib')\n",
    "# valid_smiles = valid_dataset.ids\n",
    "# test_dataset = dc.utils.load_from_disk('./chembl25_strat_test.joblib')\n",
    "# test_smiles = test_dataset.ids\n",
    "print(\"Train: \",len(train_smiles))\n",
    "print(\"Valid: \",len(valid_smiles))\n",
    "print(\"Test: \",len(test_smiles))\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 1024\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "#validation set\n",
    "predicted_valid = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted_valid)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')\n",
    "\n",
    "\n",
    "# test set\n",
    "predicted_test = model.predict_from_sequences(test_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(test_smiles, predicted_test)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(test_smiles)} test SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on CHEMBL25 for 10 epochs) on the test/validation SMILES from CHEMBL25 dataset.\n",
    "Training set: 1,428,327 \n",
    "Test set: 178,541 \n",
    "Valid set: 178, 547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1428330\n",
      "Valid:  178543\n",
      "Test:  178542\n",
      "done\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "# No VT-2005 \n",
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained for 10 epochs on chembl25) and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load CHEMBL25 from molnet\n",
    "# tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "# train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "\n",
    "# Load CHEMBL25 from joblib files instead. Probably faster than molnet\n",
    "train_dataset = dc.utils.load_from_disk('./chembl25_strat_train.joblib')\n",
    "valid_dataset = dc.utils.load_from_disk('./chembl25_strat_valid.joblib')\n",
    "test_dataset = dc.utils.load_from_disk('./chembl25_strat_test.joblib')\n",
    "\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_smiles = test_dataset.ids\n",
    "print(\"Train: \",len(train_smiles))\n",
    "print(\"Valid: \",len(valid_smiles))\n",
    "print(\"Test: \",len(test_smiles))\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 1024\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/test-cuda/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "#validation set\n",
    "predicted_valid = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted_valid)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')\n",
    "\n",
    "\n",
    "# test set\n",
    "predicted_test = model.predict_from_sequences(test_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(test_smiles, predicted_test)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(test_smiles)} test SMILES strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 1006069.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 421 of 10000 validation SMILES strings\n"
     ]
    }
   ],
   "source": [
    "#validation set\n",
    "valid_smiles = valid_smiles\n",
    "predicted_valid = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted_valid)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')\n",
    "\n",
    "\n",
    "# test set\n",
    "test_smiles = test_dataset.ids\n",
    "predicted_test = model.predict_from_sequences(test_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(test_smiles, predicted_test)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(test_smiles)} test SMILES strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.predict_embeddings(valid_smiles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_smiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 256)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_embeddings('CC').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 0 of 500 validation SMILES strings\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict_from_sequences(valid_smiles[0:500])\n",
    "count = 0\n",
    "for s,p in zip(valid_smiles[0:500], predicted):\n",
    "  if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles[0:500])} validation SMILES strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CID-SMILES\r\n",
      "VT-2005_Sigma_Profile_Database_Index_v2.xls\r\n",
      "VT-2005_Sigma_Profiles_v2\r\n",
      "VT_2005_sigma_smiles_cid.csv\r\n",
      "melting_point.csv\r\n",
      "melting_point_small.csv\r\n",
      "patents_ochem_enamine_bradley_begstrom_training_.csv\r\n",
      "pubchem_smiles.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(=O)OC(CC(=O)O)C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1=CC(C(C(=C1)C(=O)O)O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(CN)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C(C(=O)COP(=O)(O)O)N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677696</th>\n",
       "      <td>C1[C@@H]([C@H](O[C@H]1N2C3=C(C4=NC=CN4C=N3)NC2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677697</th>\n",
       "      <td>CC(=O)O[C@H]1CO[C@@H]([C@H]([C@@H]1OC(=O)C)O)C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677698</th>\n",
       "      <td>C/C(=C\\C=C\\C(=C\\C=C\\C=C(/C)\\C=C\\C=C(\\C=C\\C=C(/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677699</th>\n",
       "      <td>C=[S@@](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H](...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677700</th>\n",
       "      <td>CC1=C(N=C2C=C(C=CC2=N1)OC)O[C@@H]3C[C@H]4C(=O)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110661493 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      SMILES\n",
       "1                           CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
       "2                              CC(=O)OC(CC(=O)O)C[N+](C)(C)C\n",
       "3                                   C1=CC(C(C(=C1)C(=O)O)O)O\n",
       "4                                                    CC(CN)O\n",
       "5                                       C(C(=O)COP(=O)(O)O)N\n",
       "...                                                      ...\n",
       "162677696  C1[C@@H]([C@H](O[C@H]1N2C3=C(C4=NC=CN4C=N3)NC2...\n",
       "162677697  CC(=O)O[C@H]1CO[C@@H]([C@H]([C@@H]1OC(=O)C)O)C...\n",
       "162677698  C/C(=C\\C=C\\C(=C\\C=C\\C=C(/C)\\C=C\\C=C(\\C=C\\C=C(/...\n",
       "162677699  C=[S@@](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H](...\n",
       "162677700  CC1=C(N=C2C=C(C=CC2=N1)OC)O[C@@H]3C[C@H]4C(=O)...\n",
       "\n",
       "[110661493 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = \"../datasets/CID-SMILES\"\n",
    "data = pd.read_csv(file, sep='\\s+', header=None, names=[\"SMILES\"])\n",
    "data = pd.DataFrame(data, columns=[\"SMILES\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../datasets/pubchem_smiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dc.data.CSVLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    }
   ],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer=\"GraphConv\")\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deepchem.models' has no attribute 'GraphConvModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3bcb7f860275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphConvModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deepchem.models' has no attribute 'GraphConvModel'"
     ]
    }
   ],
   "source": [
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset,nb_epoch=5)\n",
    "# for train in train_dataset.iterbatches(batch_size=100, epochs=10, deterministic=False):\n",
    "#     model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 879: expected 26 fields, saw 28\\nSkipping line 1317: expected 26 fields, saw 27\\nSkipping line 1318: expected 26 fields, saw 27\\nSkipping line 1319: expected 26 fields, saw 27\\nSkipping line 2017: expected 26 fields, saw 28\\nSkipping line 3688: expected 26 fields, saw 27\\nSkipping line 3836: expected 26 fields, saw 27\\nSkipping line 3838: expected 26 fields, saw 27\\nSkipping line 9210: expected 26 fields, saw 27\\nSkipping line 11121: expected 26 fields, saw 27\\nSkipping line 11242: expected 26 fields, saw 28\\nSkipping line 12638: expected 26 fields, saw 27\\nSkipping line 12639: expected 26 fields, saw 27\\nSkipping line 14751: expected 26 fields, saw 29\\nSkipping line 15499: expected 26 fields, saw 28\\nSkipping line 15673: expected 26 fields, saw 28\\nSkipping line 15674: expected 26 fields, saw 28\\nSkipping line 17568: expected 26 fields, saw 27\\nSkipping line 17746: expected 26 fields, saw 27\\nSkipping line 18981: expected 26 fields, saw 27\\nSkipping line 23404: expected 26 fields, saw 28\\nSkipping line 23405: expected 26 fields, saw 28\\nSkipping line 23406: expected 26 fields, saw 28\\nSkipping line 23407: expected 26 fields, saw 28\\nSkipping line 23408: expected 26 fields, saw 28\\nSkipping line 26645: expected 26 fields, saw 29\\nSkipping line 28640: expected 26 fields, saw 29\\nSkipping line 30264: expected 26 fields, saw 27\\nSkipping line 31605: expected 26 fields, saw 32\\n'\n",
      "b'Skipping line 34273: expected 26 fields, saw 32\\nSkipping line 38032: expected 26 fields, saw 27\\nSkipping line 38033: expected 26 fields, saw 28\\nSkipping line 40016: expected 26 fields, saw 28\\nSkipping line 40017: expected 26 fields, saw 28\\nSkipping line 40018: expected 26 fields, saw 28\\nSkipping line 41799: expected 26 fields, saw 27\\nSkipping line 41800: expected 26 fields, saw 27\\nSkipping line 45118: expected 26 fields, saw 33\\nSkipping line 45119: expected 26 fields, saw 33\\nSkipping line 51131: expected 26 fields, saw 28\\nSkipping line 52066: expected 26 fields, saw 29\\nSkipping line 52067: expected 26 fields, saw 29\\nSkipping line 52068: expected 26 fields, saw 29\\nSkipping line 52069: expected 26 fields, saw 29\\nSkipping line 52070: expected 26 fields, saw 29\\nSkipping line 52071: expected 26 fields, saw 29\\nSkipping line 52357: expected 26 fields, saw 29\\nSkipping line 59721: expected 26 fields, saw 27\\nSkipping line 59722: expected 26 fields, saw 27\\nSkipping line 63404: expected 26 fields, saw 32\\n'\n",
      "b'Skipping line 67842: expected 26 fields, saw 28\\nSkipping line 69684: expected 26 fields, saw 27\\nSkipping line 69685: expected 26 fields, saw 27\\nSkipping line 69898: expected 26 fields, saw 27\\nSkipping line 70019: expected 26 fields, saw 27\\nSkipping line 72252: expected 26 fields, saw 27\\nSkipping line 72253: expected 26 fields, saw 28\\nSkipping line 72254: expected 26 fields, saw 28\\nSkipping line 73917: expected 26 fields, saw 27\\nSkipping line 73919: expected 26 fields, saw 27\\nSkipping line 73961: expected 26 fields, saw 27\\nSkipping line 73962: expected 26 fields, saw 27\\nSkipping line 73965: expected 26 fields, saw 27\\nSkipping line 73966: expected 26 fields, saw 28\\nSkipping line 73967: expected 26 fields, saw 28\\nSkipping line 73968: expected 26 fields, saw 27\\nSkipping line 74463: expected 26 fields, saw 28\\nSkipping line 74465: expected 26 fields, saw 28\\nSkipping line 76622: expected 26 fields, saw 28\\nSkipping line 79288: expected 26 fields, saw 27\\nSkipping line 79290: expected 26 fields, saw 27\\nSkipping line 79291: expected 26 fields, saw 28\\nSkipping line 79293: expected 26 fields, saw 28\\nSkipping line 79294: expected 26 fields, saw 27\\nSkipping line 79295: expected 26 fields, saw 27\\nSkipping line 82609: expected 26 fields, saw 29\\nSkipping line 82610: expected 26 fields, saw 28\\nSkipping line 82612: expected 26 fields, saw 29\\nSkipping line 82615: expected 26 fields, saw 28\\nSkipping line 82616: expected 26 fields, saw 28\\nSkipping line 82618: expected 26 fields, saw 28\\nSkipping line 82619: expected 26 fields, saw 28\\nSkipping line 82620: expected 26 fields, saw 28\\nSkipping line 82621: expected 26 fields, saw 28\\nSkipping line 82622: expected 26 fields, saw 28\\nSkipping line 82623: expected 26 fields, saw 28\\nSkipping line 82625: expected 26 fields, saw 28\\nSkipping line 82626: expected 26 fields, saw 28\\nSkipping line 82630: expected 26 fields, saw 28\\nSkipping line 82631: expected 26 fields, saw 28\\nSkipping line 82633: expected 26 fields, saw 28\\nSkipping line 82635: expected 26 fields, saw 28\\nSkipping line 82636: expected 26 fields, saw 28\\nSkipping line 82638: expected 26 fields, saw 28\\nSkipping line 82640: expected 26 fields, saw 28\\nSkipping line 82642: expected 26 fields, saw 28\\nSkipping line 82644: expected 26 fields, saw 28\\nSkipping line 82645: expected 26 fields, saw 28\\nSkipping line 82647: expected 26 fields, saw 28\\nSkipping line 82649: expected 26 fields, saw 28\\nSkipping line 90422: expected 26 fields, saw 27\\nSkipping line 91133: expected 26 fields, saw 27\\nSkipping line 92611: expected 26 fields, saw 28\\nSkipping line 93183: expected 26 fields, saw 29\\nSkipping line 93972: expected 26 fields, saw 27\\nSkipping line 94844: expected 26 fields, saw 28\\nSkipping line 94845: expected 26 fields, saw 28\\nSkipping line 96257: expected 26 fields, saw 29\\nSkipping line 96258: expected 26 fields, saw 30\\nSkipping line 96259: expected 26 fields, saw 28\\nSkipping line 96260: expected 26 fields, saw 28\\nSkipping line 96261: expected 26 fields, saw 29\\nSkipping line 96262: expected 26 fields, saw 29\\nSkipping line 96263: expected 26 fields, saw 28\\nSkipping line 96264: expected 26 fields, saw 28\\nSkipping line 96835: expected 26 fields, saw 28\\nSkipping line 96836: expected 26 fields, saw 28\\nSkipping line 98012: expected 26 fields, saw 28\\nSkipping line 98057: expected 26 fields, saw 27\\nSkipping line 98063: expected 26 fields, saw 27\\n'\n",
      "b'Skipping line 104866: expected 26 fields, saw 27\\nSkipping line 106019: expected 26 fields, saw 27\\nSkipping line 106020: expected 26 fields, saw 27\\nSkipping line 106714: expected 26 fields, saw 27\\nSkipping line 109652: expected 26 fields, saw 28\\nSkipping line 110450: expected 26 fields, saw 27\\nSkipping line 117999: expected 26 fields, saw 27\\nSkipping line 120776: expected 26 fields, saw 27\\nSkipping line 123631: expected 26 fields, saw 27\\nSkipping line 127470: expected 26 fields, saw 27\\nSkipping line 127471: expected 26 fields, saw 27\\nSkipping line 127472: expected 26 fields, saw 27\\nSkipping line 127473: expected 26 fields, saw 27\\nSkipping line 127474: expected 26 fields, saw 27\\nSkipping line 127475: expected 26 fields, saw 27\\nSkipping line 127476: expected 26 fields, saw 27\\nSkipping line 127477: expected 26 fields, saw 27\\nSkipping line 127478: expected 26 fields, saw 27\\nSkipping line 127479: expected 26 fields, saw 27\\nSkipping line 127480: expected 26 fields, saw 27\\nSkipping line 127481: expected 26 fields, saw 27\\nSkipping line 127483: expected 26 fields, saw 28\\nSkipping line 128334: expected 26 fields, saw 32\\n'\n",
      "b'Skipping line 135857: expected 26 fields, saw 28\\nSkipping line 135861: expected 26 fields, saw 27\\nSkipping line 135883: expected 26 fields, saw 27\\nSkipping line 136381: expected 26 fields, saw 28\\n'\n",
      "b'Skipping line 237268: expected 26 fields, saw 30\\n'\n",
      "b'Skipping line 274456: expected 26 fields, saw 33\\nSkipping line 274512: expected 26 fields, saw 28\\nSkipping line 274591: expected 26 fields, saw 34\\nSkipping line 274602: expected 26 fields, saw 30\\nSkipping line 274624: expected 26 fields, saw 30\\nSkipping line 274789: expected 26 fields, saw 28\\nSkipping line 274837: expected 26 fields, saw 33\\n'\n",
      "/home/AD/ulab222/anaconda3/envs/deepchem-tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (7,14,15,16,17,18,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# clean up original csv and save \n",
    "import pandas as pd\n",
    "\n",
    "file ='patents_ochem_enamine_bradley_begstrom_training_.csv'\n",
    "data = pd.read_csv(file,sep=',', header=0,error_bad_lines=False, names=[\"SMILES\",\"CASRN\",\"EXTERNALID\",\"N\",\"NAME1\",\"NAME2\",\"ARTICLEID\",\"PUBMEDID\",\"PAGE\",\"TABLE\",\"Melting Point\",\"UNIT1 {Melting Point}\",\"Melting Point {measured, converted}\",\"UNIT2 {Melting Point}\",\"Dataset\",\"comment (property)\",\"comment (chemical)\",\"measurement method\",\"Pressure\",\"UNIT {Pressure}\",\"Resp set numeric condition\",\"UNIT {Resp set numeric condition}\",\"Som set numeric condition\",\"UNIT {Som set numeric condition}\",\"Quality code\",\"UNIT {Quality code}\"])\n",
    "data = pd.DataFrame(data)\n",
    "# data\n",
    "melting_point = data[[\"SMILES\", 'Melting Point {measured, converted}']]\n",
    "melting_point = melting_point.iloc[0:50000, :]\n",
    "melting_point.rename(columns={\"Melting Point {measured, converted}\": \"Melting Point\"}, inplace=True)\n",
    "melting_point.to_csv(\"melting point small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file ='patents_ochem_enamine_bradley_begstrom_training_.csv'\n",
    "data = pd.read_csv(file, header=0, error_bad_lines=False)\n",
    "data = pd.DataFrame(data)\n",
    "# data[['Melting Point {measured, converted}', \"UNIT {Melting Point}\"]]\n",
    "len(data[\"Melting Point {measured, converted}\"].notna())\n",
    "# mp = data['Melting Point'].to_numpy()\n",
    "# mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = [\"SMILES\",\"CASRN\",\"EXTERNALID\",\"N\",\"NAME1\",\"NAME2\",\"ARTICLEID\",\"PUBMEDID\",\"PAGE\",\"TABLE\",\"Melting Point\",\"UNIT1 {Melting Point}\",\"Melting Point {measured, converted}\",\"UNIT2 {Melting Point}\",\"Dataset\",\"comment (property)\",\"comment (chemical)\",\"measurement method\",\"Pressure\",\"UNIT {Pressure}\",\"Resp set numeric condition\",\"UNIT {Resp set numeric condition}\",\"Som set numeric condition\",\"UNIT {Som set numeric condition}\",\"Quality code\",\"UNIT {Quality code}\"]\n",
    "len(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file ='melting point small.csv'\n",
    "loader = dc.data.CSVLoader([\"Melting Point\"], feature_field=\"SMILES\", featurizer=dc.feat.ConvMolFeaturizer())\n",
    "\n",
    "dataset = loader.create_dataset(csv_file)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DiskDataset X.shape: (50000,), y.shape: (50000, 1), w.shape: (50000, 1), task_names: ['Melting Point']>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = dc.splits.RandomSplitter()\n",
    "train_set, valid_set, test_set = splitter.train_valid_test_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 09:59:16.441666: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-26 09:59:16.443527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-26 09:59:16.467209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.468186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.468228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-26 09:59:16.471295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-26 09:59:16.471366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-01-26 09:59:16.474329: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-26 09:59:16.474778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-26 09:59:16.478081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-26 09:59:16.479929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-26 09:59:16.486538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-26 09:59:16.490658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-01-26 09:59:16.491267: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-26 09:59:16.700763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.701739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.701799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-26 09:59:16.701856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-26 09:59:16.701890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-01-26 09:59:16.701922: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-26 09:59:16.701955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-26 09:59:16.701987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-26 09:59:16.702019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-26 09:59:16.702051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-26 09:59:16.705703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-01-26 09:59:16.705759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-26 09:59:18.100462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-26 09:59:18.100500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2022-01-26 09:59:18.100527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \n",
      "2022-01-26 09:59:18.100533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \n",
      "2022-01-26 09:59:18.104218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6677 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:05:00.0, compute capability: 7.5)\n",
      "2022-01-26 09:59:18.106197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7259 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:06:00.0, compute capability: 7.5)\n",
      "2022-01-26 09:59:18.106390: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "model_mp = dc.models.GraphConvModel(n_tasks=1, batch_size=50, mode='regression', dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mp.fit(train_set, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_featurizer = dc.feat.RDKitDescriptors()\n",
    "features = rdkit_featurizer(['CCC'])[0]\n",
    "for feature, descriptor in zip(features[:10], rdkit_featurizer.descriptors):\n",
    "    print(descriptor, feature)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2674cf01e429b22e44fdf835f0657fdc4e794ce879752b76e7b10dc55fa0a096"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
